{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\Anaconda3\\envs\\frog\\lib\\site-packages\\gensim\\utils.py:862: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Loaded Word_embeddings Matrix (48098, 300)\n",
      " Loaded Vocabulary Mapping (size 48098)\n",
      "\n",
      " Loaded wid corpus (all)\n",
      "\n",
      " Loaded wid corpus (train)\n",
      "\n",
      " Loaded wid corpus (dev)\n",
      "\n",
      " Loaded wid corpus (test)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "from data import *\n",
    "from nnet import Model\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "##### Savefigs \n",
    "##### REport AUC, ROC... discriminative power (optimal threshold for similarity metric)\n",
    "\n",
    "\n",
    "######################### Reproduce previous results.\n",
    "######################### Evaluate baselines (Wasserstein 1, All 1)  +  Remove 'Reformulate' (standard VAE pretraining)     +    Remove discriminative training (VAE, VAD + W2 scalar)...\n",
    "######################### STS datasets / SNLI...\n",
    "######################### Revise Paper\n",
    "\n",
    "\n",
    "        \n",
    "def build_graph(batch_size=128, padlen=40):\n",
    "    tf.reset_default_graph()\n",
    "    model = Model(embedding_weights=weights, build_decoder=False, batch_size=batch_size, padlen=padlen) # Build tensorflow graph from config\n",
    "    variables_to_save = [v for v in tf.global_variables() if 'Adam' not in v.name and 'global_step' not in v.name and 'vad' not in v.name] # Saver to save & restore all the variables.\n",
    "    saver = tf.train.Saver(var_list=variables_to_save, keep_checkpoint_every_n_hours=1.0) # CLF saver\n",
    "    return model, saver\n",
    "\n",
    "\n",
    "def subplot_hist(num, content1, content2, weights1, weights2, xmin=0., xmax=1., ymin=0., ymax=1., xlabel='', title=''):\n",
    "    plt.subplot(num)\n",
    "    n2, bins2, patches2 = plt.hist(content2, 150, facecolor='r', alpha=0.75, weights=weights2) # q1 and q2 not duplicates\n",
    "    n1, bins1, patches1 = plt.hist(content1, 75, facecolor='g', alpha=0.75, weights=weights1) # q1 and q2 duplicates\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel('Normalized counts')\n",
    "    plt.axis([xmin, xmax, ymin, ymax])\n",
    "    plt.title(title)\n",
    "    #plt.grid(True)\n",
    "\n",
    "\n",
    "def plot_hist(w2s1, w2s2, mah1, mah2, title=''):\n",
    "    # Get metric power\n",
    "    y = [-1]*len(w2s1) + [1]*len(w2s2)\n",
    "    scores = w2s1 + w2s2\n",
    "    print('W2 AUC: {}'.format(roc_auc_score(y, scores)))\n",
    "\n",
    "    y = [-1]*len(mah1) + [1]*len(mah2)\n",
    "    scores = mah1 + mah2\n",
    "    print('Mah AUC: {}'.format(roc_auc_score(y, scores)))\n",
    " \n",
    "    # Plot density like histograms (sum=1)\n",
    "    w1 = np.ones_like(w2s1)/float(len(w2s1))\n",
    "    w2 = np.ones_like(w2s2)/float(len(w2s2))\n",
    "\n",
    "    plt.rcParams['figure.figsize'] = [10, 5]\n",
    "    #plt.rcParams[\"text.color\"] = 'w'\n",
    "    #plt.rcParams[\"axes.labelcolor\"] = 'w'\n",
    "    #plt.rcParams[\"axes.edgecolor\"] = 'w'\n",
    "    #plt.rcParams[\"ytick.color\"] = 'w'\n",
    "    #plt.rcParams[\"xtick.color\"] = 'w'\n",
    "\n",
    "    plt.figure(1)\n",
    "    subplot_hist(111, np.array(w2s1), np.array(w2s2), w1, w2, xmax=500, ymax=0.15, xlabel='W2(q1,q2)', title=title) # Wasserstein 2 distance\n",
    "    #subplot_hist(212, np.array(mah1), np.array(mah2), w1, w2, xmax=500, ymax=0.1, xlabel='Mah(q1,q2)', title=title) # Mahalanobis distance\n",
    "    plt.savefig('../../insights/proposal/discriminative_power/{}.png'.format(title))\n",
    "    plt.show()\n",
    "\n",
    "  \n",
    "def eval(model, saver, Xs, Ys, Xa, Ya, pretrain='VAD', clf_nepochs=3, batch_size=128, padlen=40):\n",
    "    ns, na = len(Xs), len(Xa)\n",
    "    ratio = ns/(ns+na)\n",
    "    w2s, mahs, predictions = {'s':[],'a':[]}, {'s':[],'a':[]}, {'s':[],'a':[]}\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer()) # run init op\n",
    "        saver.restore(sess, '{}/_deepNLU/save/{}SIAM{}/actor.ckpt'.format(dir_,pretrain,clf_nepochs)) # Restore variables from disk.\n",
    "        print('\\n Restored {}SIAM{}'.format(pretrain,clf_nepochs))\n",
    "\n",
    "        ''' Eval True paraphrases '''\n",
    "\n",
    "        batches = create_batches(ns, batch_size=batch_size, shuffle=False)\n",
    "        batches.append(np.arange(np.floor(ns/batch_size).astype(int)*batch_size, ns))\n",
    "        \n",
    "        q, q_len = {}, {}\n",
    "        for i, idx_batch in enumerate(batches):\n",
    "            q1_, q2_, q1_len_, q2_len_ = [], [], [], []\n",
    "            for j in idx_batch:\n",
    "                q1_j, q1_len_j = pad_sequence(list(Xs[j]), padlen=padlen) \n",
    "                q2_j, q2_len_j = pad_sequence(list(Ys[j]), padlen=padlen)\n",
    "                q1_.append(q1_j)\n",
    "                q2_.append(q2_j)\n",
    "                q1_len_.append(q1_len_j)\n",
    "                q2_len_.append(q2_len_j)\n",
    "            q[i] = np.concatenate([np.stack(q1_, axis=0), np.stack(q2_, axis=0)], axis=0)\n",
    "            q_len[i] = np.concatenate([np.stack(q1_len_, axis=0), np.stack(q2_len_, axis=0)], axis=0)\n",
    "\n",
    "        for i in tqdm(np.arange(len(batches))):\n",
    "            feed = {model.q1: q[i], model.len1: q_len[i]}\n",
    "            W2_dist, Mah_dist, predict = sess.run([model.W2_dist, model.Mah_dist, model.predict], feed_dict=feed) # Forward pass\n",
    "            w2s['s'] += list(W2_dist)\n",
    "            mahs['s'] += list(Mah_dist)\n",
    "            predictions['s'] += list(predict)\n",
    "\n",
    "        ''' Eval False paraphrases '''\n",
    "        batches = create_batches(na, batch_size=batch_size, shuffle=False)\n",
    "        batches.append(np.arange(np.floor(na/batch_size).astype(int)*batch_size, na))\n",
    "        \n",
    "        q, q_len = {}, {}\n",
    "        for i, idx_batch in enumerate(batches):\n",
    "            q1_, q2_, q1_len_, q2_len_ = [], [], [], []\n",
    "            for j in idx_batch:\n",
    "                q1_j, q1_len_j = pad_sequence(list(Xa[j]), padlen=padlen) \n",
    "                q2_j, q2_len_j = pad_sequence(list(Ya[j]), padlen=padlen)\n",
    "                q1_.append(q1_j)\n",
    "                q2_.append(q2_j)\n",
    "                q1_len_.append(q1_len_j)\n",
    "                q2_len_.append(q2_len_j)\n",
    "            q[i] = np.concatenate([np.stack(q1_, axis=0), np.stack(q2_, axis=0)], axis=0)\n",
    "            q_len[i] = np.concatenate([np.stack(q1_len_, axis=0), np.stack(q2_len_, axis=0)], axis=0)\n",
    "\n",
    "        for i in tqdm(np.arange(len(batches))):\n",
    "            feed = {model.q1: q[i], model.len1: q_len[i]}\n",
    "            W2_dist, Mah_dist, predict = sess.run([model.W2_dist, model.Mah_dist, model.predict], feed_dict=feed) # Forward pass\n",
    "            w2s['a'] += list(W2_dist)\n",
    "            mahs['a'] += list(Mah_dist)\n",
    "            predictions['a'] += list(predict)\n",
    "\n",
    "    TP = np.mean(np.array(predictions['s']))\n",
    "    FN = np.mean(np.array(predictions['a']))\n",
    "    accuracy = ratio*TP+(1-ratio)*(1-FN)\n",
    "    print('\\n Eval COMPLETED ! {:.4f} TP ({} pairs), {:.4f} FN ({} pairs), {:.4f} accuracy ({} pairs {:.4f} bias)'.format(100*TP, ns, 100*FN, na, 100*accuracy, ns+na, 100*(1-ratio)))\n",
    "\n",
    "    return w2s, mahs, predictions  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "semantic_power = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "padlen_ = 40\n",
    "batch_size_ = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluation on Quora TEST set\n",
      "INFO:tensorflow:Restoring parameters from ../_deepNLU/save/VADSIAM3/actor.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-05 13:33:10,519 : INFO : Restoring parameters from ../_deepNLU/save/VADSIAM3/actor.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Restored VADSIAM3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|███████████████████████████████████████████████████████████████████████           | 26/30 [04:21<00:40, 10.08s/it]"
     ]
    }
   ],
   "source": [
    "for set_name_ in ['TEST']: # TEST\n",
    "    print('\\n Evaluation on Quora {} set'.format(set_name_))\n",
    "    if set_name_=='DEV':\n",
    "        Xs, Ys, Xa, Ya = Xs_dev_ids, Ys_dev_ids, Xa_dev_ids, Ya_dev_ids\n",
    "    else:\n",
    "        Xs, Ys, Xa, Ya = Xs_test_ids, Ys_test_ids, Xa_test_ids, Ya_test_ids\n",
    "\n",
    "    for pretrain_ in ['VAD']: #VAD\n",
    "        for clf_nepochs_ in [3]: # 0,3\n",
    "            model, saver = build_graph(batch_size=batch_size_, padlen=padlen_)\n",
    "            w2s, mahs, predictions = eval(model, saver, Xs, Ys, Xa, Ya, pretrain=pretrain_, clf_nepochs=clf_nepochs_, batch_size=batch_size_, padlen=padlen_) # Dev dataset\n",
    "            semantic_power['Quora{}:: {}SIAM{} '.format(set_name_, pretrain_, clf_nepochs_)] = w2s, mahs, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print / plot results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, (w,m,p) in semantic_power.items():\n",
    "    k = k.replace(':: ','')\n",
    "    print(k)\n",
    "    if '0' not in k:\n",
    "        plot_hist(w['s'], w['a'], m['s'], m['a'], title=k[5:])\n",
    "    \n",
    "    \n",
    "# Baselines: 0.78 test acc (pLSI + MLP) / 0.78 (Z, max_len 12, VAD + MLP) // 0.73 (Z, max_len 12, VAE + MLP)\n",
    "# VAD+CLF: 89.03% DEV {TP: 0.858, FN: 0.090} // 88.86% TEST {TP: 0.853, FN: 0.090} [n2000_h1000_epo5_pad40_max40_d0.6_m1000_r11e-05_r20.0_epo3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
